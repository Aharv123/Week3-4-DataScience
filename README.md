Data Cleaning and Feature Engineering

Overview

This repository contains Python code covering Data Cleaning and Feature Engineering techniques used in data preprocessing.
The implementation follows the syllabus topics from Week 8 (Data Cleaning) and Week 9 (Feature Engineering).

Week 3: Data Cleaning

Topics Covered:

Handling Missing Data

Removing missing values

Imputation methods:

Mean imputation

Median imputation

K-Nearest Neighbors (KNN) imputation

Handling Duplicates

Identifying and removing duplicate records

Handling Outliers

Using Z-score and IQR (Interquartile Range) methods

Capping and flooring technique

Data Quality Checks

Checking for inconsistent data

Identifying incorrect data types

Preprocessing Pipelines

Automating data cleaning steps using Scikit-learn pipelines

Week 4: Feature Engineering

Topics Covered:

Feature Scaling

Standardization (Z-score normalization)

Min-Max Normalization

Feature Encoding

One-hot encoding

Label encoding

Ordinal encoding

Feature Transformation

Polynomial Features

Log transformations for skewed data

Feature Selection Techniques

Variance Threshold (Removing low-variance features)

SelectKBest (Choosing the best k features)

Recursive Feature Elimination (RFE) (Eliminating less important features iteratively)

Implementation

The repository includes Jupyter notebooks demonstrating each technique with examples.

Datasets such as the Iris dataset and a custom Employee dataset have been used.

Scikit-learn and Pandas have been used extensively for preprocessing.

